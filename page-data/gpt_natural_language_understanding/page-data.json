{"componentChunkName":"component---src-templates-post-template-tsx","path":"/gpt_natural_language_understanding/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<h1>GPT는 말을 어떻게 이해할까?</h1>\n<h3>— LLM이 자연어를 처리하는 방식을 쉽게 풀어보는 이야기</h3>\n<blockquote>\n<p>“GPT는 정말 말을 이해하는 걸까?”<br>\n“어떻게 AI가 문장을 읽고, 대답까지 하는 거지?”</p>\n</blockquote>\n<p>하루가 멀다 하고 등장하는 AI 서비스들.<br>\n그 중심엔 ‘GPT’, ‘Claude’, ‘Gemini’ 같은 **LLM (Large Language Model)**이 있습니다.<br>\n그런데 이 LLM들은 <strong>사람처럼 어떻게 자연어를 이해하고 대답</strong>할 수 있을까요?</p>\n<p>오늘은 이 AI가 <strong>말을 이해하는 구조</strong>를,<br>\n<strong>사람에 빗대어 아주 쉽게 풀어보는 이야기</strong>로 시작해봅니다.</p>\n<hr>\n<h2>1. AI는 ‘언어’를 이해할까?</h2>\n<p>먼저 정리하고 시작할게요.<br>\nGPT는 인간처럼 <strong>진짜로 언어를 이해하진 않습니다.</strong><br>\n하지만 언어의 <strong>패턴과 확률</strong>, <strong>문맥과 의미 흐름</strong>을 기가 막히게 예측합니다.</p>\n<p>예를 들어:</p>\n<blockquote>\n<p>너 오늘 기분 어때?<br>\n→ “좋아” / “별로야” / “괜찮아” 등 다양한 답이 올 수 있죠.</p>\n</blockquote>\n<p>GPT는 방대한 텍스트 학습을 통해<br>\n**“이런 질문엔 보통 이런 단어들이 오더라”**는 걸<br>\n확률적으로 계산해내는 능력이 엄청납니다.</p>\n<hr>\n<h2>2. 말하는 AI의 비밀 — GPT는 ‘언어의 퍼즐’을 푸는 천재</h2>\n<h3>🧠 사람은 어떻게 말을 이해할까요?</h3>\n<ol>\n<li>글자를 보고</li>\n<li>단어를 조합하고</li>\n<li>문맥을 파악하고</li>\n<li>머릿속 배경지식을 동원해</li>\n<li>대답을 합니다.</li>\n</ol>\n<p>GPT도 비슷한 과정을 거칩니다.<br>\n다만 이 모든 걸 <strong>수학과 벡터, 확률</strong>로 처리합니다.</p>\n<hr>\n<h2>3. GPT의 작동 방식, 쉽게 풀어보자</h2>\n<h3>🔹 Step 1: 단어를 ‘쪼개기’ (Tokenizing)</h3>\n<p>AI는 사람처럼 문장을 통째로 이해하지 못합니다.<br>\n대신 단어들을 **“토큰(token)“**이라는 작은 단위로 쪼갭니다.</p>\n<blockquote>\n<p>예: “Hello, world!”<br>\n→ [‘Hello’, ’,’, ‘world’, ’!‘]</p>\n</blockquote>\n<p>이게 AI의 ‘읽기’ 방식입니다.<br>\n(심지어 ‘안녕하세요’는 [‘안’, ‘녕’, ‘하’, ‘세’, ‘요’]처럼 쪼개지기도 해요!)</p>\n<hr>\n<h3>🔹 Step 2: 단어를 숫자로 바꾸기 (Embedding)</h3>\n<p>쪼개진 단어들은 이제 숫자(벡터)로 바뀝니다.<br>\n예를 들어 “사과”는 [0.1, 0.8, -0.2, …] 같은 벡터가 됩니다.</p>\n<p>이 벡터는 <strong>단어의 의미</strong>를 숫자 공간에서 표현한 거예요.<br>\n그래서 AI는</p>\n<ul>\n<li>‘사과’와 ‘과일’은 가까이 있고</li>\n<li>‘사과’와 ‘자동차’는 멀리 있다는 걸</li>\n</ul>\n<p>벡터 거리로 판단합니다.</p>\n<hr>\n<h3>🔹 Step 3: 문맥 이해 (Transformer 구조)</h3>\n<p>이제 GPT의 진짜 두뇌, <strong>Transformer</strong>의 출동입니다.</p>\n<p>Transformer는 다음을 고려합니다:</p>\n<ul>\n<li><strong>문맥</strong>: 앞뒤 문장을 다 살펴봄</li>\n<li><strong>순서</strong>: 어떤 단어가 먼저 나왔는지</li>\n<li><strong>중요도</strong>: 어떤 단어가 핵심인지 강조</li>\n</ul>\n<p>이걸 가능하게 하는 게 바로 <strong>Attention(주의집중) 메커니즘</strong>입니다.</p>\n<blockquote>\n<p>“나는 오늘 회사에 갔는데, 거기서 고양이를 만났어.”</p>\n</blockquote>\n<p>여기서 ‘거기서’는 ‘회사’를 가리키는 거죠.<br>\nGPT는 이런 연결을 <strong>확률과 가중치</strong>로 파악합니다.</p>\n<hr>\n<h3>🔹 Step 4: 다음 단어 예측</h3>\n<p>AI는 위 과정을 거쳐<br>\n**“지금까지 이 말을 봤을 때, 다음 단어로 올 확률이 가장 높은 건?”**을 계산합니다.</p>\n<p>예:</p>\n<blockquote>\n<p>“비가 오는 날에는” →<br>\n예측: [‘우산을’, ‘집에’, ‘밖에’, ‘커피를’…]</p>\n</blockquote>\n<p>그 중에서 가장 그럴싸한 걸 하나씩 택해 문장을 이어갑니다.</p>\n<p>이 과정을 수천 번 반복하면,<br>\nGPT는 하나의 <strong>매끄러운 글, 대화, 요약, 번역</strong>을 완성해냅니다.</p>\n<hr>\n<h2>4. 그럼 GPT는 ‘기억’도 하나요?</h2>\n<p>아니요. 기본적으로는 <strong>기억하지 못합니다.</strong><br>\n대화도 한 번에 넣는 <strong>입력 범위(토큰 수)</strong> 안에서만 기억해요.</p>\n<p>다만, <strong>ChatGPT처럼 대화를 이어가는 시스템</strong>에서는<br>\n이전 대화 내용을 계속 다시 보내주기 때문에<br>\n‘기억하는 것처럼’ 느껴지는 거랍니다.</p>\n<hr>\n<h2>5. 결국 중요한 건 “확률”과 “패턴”</h2>\n<p>GPT는 인간처럼 이해하거나 느끼지는 않지만,<br>\n<strong>수십억 개 문장의 패턴과 확률</strong>을 학습해<br>\n놀랍도록 자연스러운 언어 생성이 가능해진 겁니다.</p>\n<hr>\n<h2>✨ 마무리하며 — AI는 생각하지 않지만, 생각처럼 보이게 한다</h2>\n<p>GPT는 인간의 뇌처럼 <strong>이해</strong>하진 않지만,<br>\n말을 조합하는 <strong>확률적 사고</strong>는 어쩌면 인간보다 뛰어납니다.</p>\n<p>우리가 이 원리를 알면,<br>\nGPT를 더 잘 활용하고,<br>\n어디까지 가능하고 어디서부터 조심해야 하는지도 판단할 수 있습니다.</p>\n<blockquote>\n<p><strong>“AI는 마법이 아닙니다. 수학과 데이터, 그리고 확률의 예술이죠.”</strong></p>\n</blockquote>","frontmatter":{"title":"AI의 LLM 처리 하는방안..?","summary":"AI의 독해능력","date":"2025.07.09.","categories":["AI"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAERUlEQVR42j1S2U/idxDnL+lbn/rWZB/60qRN9qFNmmzTdKtd27qtB0XQ1XUVRBfN4q2wolGIiogKigiCHHIIAnKI3Df8uPHiUNhdxSvb7Xi0ySS/+c13PjOfmc+gLnKRci5ydRI7OwodRvYidm0maCllfBCBOLxeFWKlfX8h6SpmfJeFaPku/x6FKmfDV4VoNmp36iWIU5tPOuMevcckzwTMkHqZRw7CVrdRFnVpveZNqAvJAHkAXxaQ07QXHsL2rYBV5TRIw3bNUcIR8+iPo/br03jYrk0GjLZtUTpojrn1QPAihzyAr09icY/hELEmfTsW9ZrTIJFwGQtTQ6cZd8JnBMLBPXU6YAI6+yFLYFdVzkYu8sg9cxTMhrj1uaQz4dVDXtCm1kk53S/RmZDFY1E4dBsugzQXd0JPxKG9KiWvi4li2gOLgCq3ndMBM7SF8nad2LIlcO0qVOvzZ9lQyKYxKfkOvcRlkL1px9ZVPmnHVEMw5jacpDywDhQQODsO+S1Kn0URdW1DCb9VCRPC/Cm/6f1hMB20VHz39eNHX/zy+Kufv31U8f03tm0x6ALrRJ1nw/D5cBSMOLT+XRWAIw5N0Kpym+Qhuybm0sXcO2pWf3STntEy99aoX37+GX9u/NP5AWzutvP9Dm6KcVDylqqCD7UCeyqoYtdtlPYDJv54VD6ZUE0nVdPUtucOrfDju9T58R0YmgMYNPRZVEmfEXywQtKd9BpBp9JBANliBcS0soP3j0/oltBTvp1LOKFs+K5zDgE94QcA2bizmPHenMaBCARhK+c5xCGbs/GG86ZFRDbl3ZF8Ot+H+C0Y5AKSB6HdY8Tm31WCNjDnUcQKROB4boqJfNxpXRsXTRC8AopfTBsYHJSI1k6SLpAJ9bGUsOskQg5TLljaFHJUGzzFOlcu4Gzw2JIV5vujMG+WljUyuVT87FjfwNBwfSf1eWvvm0FKMe29A+ulUgFXIxcqRCsbvAW5kCtfX1ZLBausydXFWXxbu2J+tJnYW9c11tBNaySNYgl9aOLInl6OgvFSAbOAw9QrxWzGGK66gk7tZ7wdMG7JONM0DAbbQBqrbups7KE1vwGb+BtPxnWNYF9TzBopCg4NzkXImdUpxRQyEV31U1cLhtrbZVBJ+PP0+iZ8A+ntny9ItUQKjkRt7BlDv+pp6BzGdo4E7ToARz8cBlRirla+LlpmyQRLq2wGOEBkU8glvyYsUDvVS9SJ0f761m4MvhfdSsIQh1p7RvMJJwqUuHmX2pbxWQwab2GGy2KssGeW2dPgLDLpQ8Smuf4W5SzZsDSMxvdhusdxPeM1HZRBCq2cj6DWqATRBGmht3mSUEcnoqf+M/qd0V79NYGvpbXVjDb/Tqp7Ssb+NtBSi6+pHH7xx+pIG2qJjJvGPWG3VS62//pg+Gf/+0v4Z2AcQhXYckfVSkcV8+XTKeyPdMwPM8TafwFDhGCTATEZUwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/a6aafa201d3364cfe4040cdee435c13c/34104/LLMAI.png","srcSet":"/static/a6aafa201d3364cfe4040cdee435c13c/5ce94/LLMAI.png 256w,\n/static/a6aafa201d3364cfe4040cdee435c13c/8a20a/LLMAI.png 512w,\n/static/a6aafa201d3364cfe4040cdee435c13c/34104/LLMAI.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/a6aafa201d3364cfe4040cdee435c13c/99d55/LLMAI.webp 256w,\n/static/a6aafa201d3364cfe4040cdee435c13c/3591c/LLMAI.webp 512w,\n/static/a6aafa201d3364cfe4040cdee435c13c/e306d/LLMAI.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}},"publicURL":"/static/a6aafa201d3364cfe4040cdee435c13c/LLMAI.png"}}}}]}},"pageContext":{"slug":"/gpt_natural_language_understanding/"}},"staticQueryHashes":[],"slicesMap":{}}